{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_punctuation = '-!\"$%^&*()_+=~:;,<.>?/[\\\\]\\`{|}@'\n",
    "def remove_links(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_unicode_chars(text):\n",
    "    encoded_string = text.encode(\"ascii\", \"ignore\")\n",
    "    decode_string = encoded_string.decode()\n",
    "    return decode_string\n",
    "\n",
    "def clean_text(text):\n",
    "    text = remove_links(text)\n",
    "    text = remove_unicode_chars(text)\n",
    "    text = text.lower()\n",
    "    text = re.sub('['+my_punctuation + ']+', ' ', text)\n",
    "    text = text.replace('\\\\', '')\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = re.sub('([0-9]+)', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def define_stop_words():\n",
    "    #Custom stopwords\n",
    "    custom_stopwords = ['come','order','try','go','get','make','drink','plate','dish','restaurant','place']\n",
    "    \n",
    "    #Customize stop words\n",
    "    #spacy_stop_words = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "    \n",
    "    # ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
    "    #ALL_STOP_WORDS = spacy_stop_words.union(SW).union(stopwords)\n",
    "    \n",
    "\n",
    "    # NLTK Stop words\n",
    "    stop_words = nltk_SW.words('english')\n",
    "    stop_words.extend(custom_stopwords)#.extend(SW).extend(stopwords)\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def remove_stopwords(text, tokenizer):\n",
    "    #Custom stopwords\n",
    "    custom_stopwords = ['come','order','try','go','get','make','drink','plate','dish','restaurant','place']\n",
    "    \n",
    "    #Customize stop words\n",
    "    spacy_stop_words = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "    \n",
    "    # ALL_STOP_WORDS = spacy + gensim + wordcloud\n",
    "    ALL_STOP_WORDS = spacy_stop_words.union(SW).union(stopwords)\n",
    "    \n",
    "    tokens = []\n",
    "    \n",
    "    for doc in tokenizer.pipe(text, batch_size=500):\n",
    "        doc_tokens = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() not in ALL_STOP_WORDS:\n",
    "                doc_tokens.append(token.text.lower())\n",
    "        tokens.append(doc_tokens)\n",
    "        \n",
    "    return tokens\n",
    "\n",
    "'''\n",
    "\n",
    "def get_lemmas(text):\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for token in doc:\n",
    "        if ((token.is_stop == False) and (token.is_punct == False) and (token.pos_ != 'PRON')):\n",
    "            lemmas.append(token.lemma_)\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "def generate_lemma_tokens(df):\n",
    "    tokenizer = Tokenizer(nlp.vocab)\n",
    "    \n",
    "    # remove stop words and generate tokens\n",
    "    df['tokens'] = remove_stopwords(df['clean_text'], tokenizer)\n",
    "    \n",
    "    df['tokens_to_text'] = [' '.join(map(str, l)) for l in df['tokens']]\n",
    "    \n",
    "    df['lemmas'] = df['tokens_to_text'].apply(get_lemmas)\n",
    "    \n",
    "    return df\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    ALL_STOP_WORDS = define_stop_words()\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in ALL_STOP_WORDS] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts, bigram_mod):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts,trigram_mod):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_lemma_text(clean_text):\n",
    "    data = clean_text.values.tolist()\n",
    "    \n",
    "    # Tokenization\n",
    "    data_words = list(sent_to_words(data))\n",
    "    \n",
    "    \n",
    "    # Build the bigram and trigram models\n",
    "    bigram = gensim.models.Phrases(data_words, min_count=15, threshold=100) # higher threshold fewer phrases.\n",
    "    trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "    # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "    \n",
    "    # Remove Stop Words\n",
    "    data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "    # Form Bigrams\n",
    "    data_words_bigrams = make_bigrams(data_words_nostops, bigram_mod)\n",
    "\n",
    "    # Do lemmatization keeping only noun, adj, vb, adv\n",
    "    text_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "    \n",
    "    return text_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
